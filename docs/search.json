[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Hello there!",
    "section": "",
    "text": "I am a Computational Social Science student with an excitement for statistics, programming and the use of computational methods in social scientific research. I have a professional background in quantitative media research and data analysis. Go check out my blog if you want to know what I’m working on at the moment. If you want to know more about me, check out my CV or my publications."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "blog/index.html#section",
    "href": "blog/index.html#section",
    "title": "Blog",
    "section": "2024",
    "text": "2024\n\n\n    \n    \n                  \n            September 2, 2024\n        \n        \n            The magic of Fixed Effects regression\n\n            \n            \n                \n                \n                    Econometrics\n                \n                \n                \n                    Statistical methods\n                \n                \n                \n                    R\n                \n                \n            \n            \n\n            In this article I give a short introduction to Fixed Effects regression. I aim to provide an intuition for how it works and the scenarios in which it is most useful\n            \n            \n        \n        \n        \n            \n        \n        \n    \n    \n    \n                  \n            May 23, 2024\n        \n        \n            How to set up a web scraping environment with R and Selenium on Ubuntu\n\n            \n            \n                \n                \n                    Web scraping\n                \n                \n                \n                    Linux\n                \n                \n                \n                    R\n                \n                \n            \n            \n\n            Learn in this tutorial how to create a simple and beginner-friendly web scraping environment with Selenium and R on Ubuntu\n            \n            \n        \n        \n        \n            \n        \n        \n    \n    \n\n\nNo matching items"
  },
  {
    "objectID": "blog/index.html",
    "href": "blog/index.html",
    "title": "Decode the Digits",
    "section": "",
    "text": "Welcome to my blog Decode the Digits. Add more description here."
  },
  {
    "objectID": "blog/article1_tba/article1.html",
    "href": "blog/article1_tba/article1.html",
    "title": "First test article",
    "section": "",
    "text": "CitationBibTeX citation:@online{2024,\n  author = {},\n  title = {First Test Article},\n  date = {2024-05-12},\n  doi = {10.59350/pe5s8-e0f47},\n  langid = {en}\n}\nFor attribution, please cite this work as:\n“First Test Article.” 2024. May 12, 2024. https://doi.org/10.59350/pe5s8-e0f47."
  },
  {
    "objectID": "blog/article1_tba/article1.html#title-1",
    "href": "blog/article1_tba/article1.html#title-1",
    "title": "First test article",
    "section": "",
    "text": "Here comes the content"
  },
  {
    "objectID": "blog/2024/article1_tba/index.html",
    "href": "blog/2024/article1_tba/index.html",
    "title": "How to set up a web scraping environment with R and Selenium on Ubuntu",
    "section": "",
    "text": "Preface: Required software\nIn this tutorial I use Ubuntu as the operating system. I assume that many of the steps described in the following can also be transferred to other (especially Unix-based) operating systems. However, since I have not yet tried it on other operating systems, I can only “guarantee” that my instructions will work for Ubuntu (I am using 22.04).\nApart from an operating system, you also need an installation of R and the Docker Engine for Ubuntu. For writing code in R, I also recommend using a suitable IDE such as RStudio. If you already have these installed on your system, you can get started right away and pull a Docker image from Selenium.\nIf you don’t have R installed yet, you can install it from one of the CRAN mirrors in your country. You can install RStudio here. To install the Docker Engine on your machine, follow the instructions on the docker website. Simply copy the code given on the page into a terminal window and you should be good to go.\nAfter you have installed all the necessary software, we can move on to the actual tool we will use to automate our browser: Selenium.\n\n\nPull docker image of Selenium\nSelenium is a software with which we can automate a browser. Many of the functions that Selenium contains are therefore extremely helpful for scraping content from websites. In this tutorial, we will not install Selenium directly on our system, but run an image of Selenium within a Docker container.\nTo be able to use Selenium within a container, we first have to pull an image of it in the Docker Engine. Depending on which browser you want to use, you can pull different versions for different browsers. Standalones are available for Firefox, Chrome, and Edge.\nSince I personally prefer to use Firefox, I’ll install a version that “uses” firefox. With the following command in the terminal, you can pull the image:\n\ndocker pull selenium/standalone-firefox \n\n\n\n\n\n\n\nIssues with pulling the image?\n\n\n\nWhen I first tried to pull the image of Selenium, I got the error message: denied while trying to connect to the Docker daemon socket at unix\nThis error usually occurs when the user that runs the command to pull the image doesn’t have the necessary permissions to access the Docker daemon.\nFollow these steps to fix this problem permanently:\n\nAdd your user to the Docker group, which grants the user permission to interact with the Docker daemon. You can do this with the following command:\n\n\nsudo usermod -aG docker $USER \n\n\nAfter adding your user to the Docker group, you can run the following command to apply the changes to the current session: newgrp docker\nAfter logging back in, you can verify that your user has access to Docker by running a Docker command without sudo, such as: docker ps\nTry again to pull the docker image\n\n\n\n\n\nSet up a Docker container with Selenium\nAfter we have successfully pulled the Selenium image, we can now create a Docker container in which we run Selenium. To do so, use the following command in the terminal:\n\ndocker run -d -p 4444:4444 -p 7900:7900 --shm-size=\"2g\" selenium/standalone-firefox:latest \n\nThis command creates and starts a new Docker container based on the latest selenium/standalone-firefox image.\n\n Are you interested in what all the elements of this command mean? Click here to deconstruct it.\n \n\n  \n\n    docker run: This is the command used to create and start a new Docker container based on a specified Docker image\n    -d: This activates the \"detached mode.\" It tells Docker to run the container in the background and print the container ID\n    -p 4444:4444: This option maps port 4444 on the host machine to port 4444 inside the Docker container. This allows communication with the Selenium server from outside the container\n    -p 7900:7900: Similar to the previous option, this maps port 7900 on the host machine to port 7900 inside the Docker container. This port is used for Virtual Network Computing (VNC), which later allows us to see what our web scraper sees when navigating the Internet\n    --shm-size=\"2g\": This option sets the size of the shared memory segment for the container. In this case, it's set to 2 gigabytes (2g)\n    selenium/standalone-firefox:latest: This is the Docker image name and tag. It specifies the image to use for creating the container. Here, we are using the selenium/standalone-firefox image. The :latest tag indicates that it's using the latest version of the image.\n\n  \n\n\nWe have now managed to complete all preparations so that the web scraping can begin. Before we can control our web scraper in R, however, we always have to start the Docker container with the Selenium image first.\n\n\nStart (and stop) the Selenium Docker container\nTo know whether we need to start our Docker container with Selenium, it usually makes sense to get an overview of all active Docker containers. To get an overview of all running containers, use this command in the terminal:\n\ndocker ps\n\nIf you also want to see containers that are not active at the moment but ran in the past, use the command:\n\ndocker ps -a\n\nThe second command comes really handy if you want to start a container that you have already used in the past. Because then you can simply copy the ID of the desired container from the overview of already used containers and then restart the old container with the following command:\n\ndocker start &lt;here ID of desired container&gt;\n\n\n\n\n\n\n\nStopping a container\n\n\n\nI advise you to stop the container after you finished the scraping. A running container requires computing power from your system and may cause other processes on the system to slow down.\nTo stop the container, use the command:\n\ndocker stop &lt;ID of running container&gt;\n\n\n\n\n\nControl the scraper in R\nOnce our Docker container with Selenium is running, we can start programming our web scraper in R. To do so, we use the R package selenium, which you can install as usual with install.packages(“selenium”) in R. This package contains numerous functions with which we can control the behavior of the automated browser. Explaining every function would probably require a separate blog entry - so I will only give a rough overview of the most important functions here:\n\nSeleniumSession$new(browser = \"firefox\") -&gt; object # starts the automated browser (here we need to assign the output of the function to an R object which we will use to address the browser in further instructions) \n\nobject$navigate(\"insert here an URL\") # navigates the browser to an URL\n\nobject$find_element # helps us to address a specific element on a webpage (for example with an XPATH)\n\nobject$get_page_source() # retrieves the source code of the webpage\n\n\n\n\n\n\n\nListen on ports to see what our scraper is doing\n\n\n\nIn most scraping projects, the automated browser has to click through various elements or make inputs at specific points on the website. Therefore, it is helpful to see how the automated browser navigates through a site so that potential barriers can be easily observed and taken into account when programming the scraper. The great thing about Selenium is that it comes with a built-in VNC that allows us to do just that. All we need to do is type the following URL into a browser:\nhttp://localhost:7900/?autoconnect=1&resize=scale&password=secret\nIn addition, we can obtain an overview of the active automated browsers when listening on port 4444. This can be interesting if you want to run several scrapers simultaneously. To listen on port 4444, type in your browser:\nhttp://localhost:4444\n\n\nIf you have any questions or feedback about this blog post, please feel free to send me an email. Otherwise, I wish you a happy scraping!"
  },
  {
    "objectID": "publications.html",
    "href": "publications.html",
    "title": "Publications",
    "section": "",
    "text": "In the following, you’ll find an overview of reports and articles I contributed to as an author."
  },
  {
    "objectID": "publications.html#section",
    "href": "publications.html#section",
    "title": "Publications",
    "section": "2023",
    "text": "2023\n\nCousseran, L., Lauber, A., Herrmann, S., & Brüggen, N. (2023). Compass: Artificial intelligence and competence 2023. Attitudes, actions and competence development in the context of AI. Munich: kopaed. https://doi.org/10.5281/ZENODO.10058587\nHerrmann, S., Lauber, A., Cousseran, L., & Brüggen, N. (2023). Compass: Artificial intelligence and competence 2022. User types of interaction with AI. Munich: kopaed. https://doi.org/10.5281/ZENODO.8245676\nHerrmann, S., Cousseran, L., Tausche, S., Pfaff-Rüdiger, S., & Brüggen, N. (2023). Compass: Artificial intelligence and competence 2022. Media use and attitudes towards AI. Munich: kopaed. https://doi.org/10.5281/ZENODO.6900551"
  },
  {
    "objectID": "publications.html#section-1",
    "href": "publications.html#section-1",
    "title": "Publications",
    "section": "2022",
    "text": "2022\n\nSchober, M., Lauber, A., Bruch, L., Herrmann, S., & Brüggen, N. (2022). “What I like comes to me”. Young people’s competencies in using algorithmic recommendation systems. Munich: kopaed. https://doi.org/10.5281/ZENODO.7437429\nPfaff-Rüdiger, S., Herrmann, S., Cousseran, L., & Brüggen, N. (2022). Compass: Artificial intelligence and competence 2022. Knowledge and action in the context of AI. Munich: kopaed. https://doi.org/10.5281/ZENODO.6668912\n\n(All publication titles were translated from German into English)"
  },
  {
    "objectID": "blog/2024/article1_tba/index.html#heading-2",
    "href": "blog/2024/article1_tba/index.html#heading-2",
    "title": "First test article",
    "section": "",
    "text": "docker run -d -p 4444:4444 -p 7900:7900 --shm-size=\"2g\" selenium/standalone-firefox:latest \n\n\n\n\n\n\nPossible issues with pulling image for the first time\n\n\n\n\nAdd text here"
  },
  {
    "objectID": "blog/2024/article1_tba/index.html#heading-2-1",
    "href": "blog/2024/article1_tba/index.html#heading-2-1",
    "title": "First test article",
    "section": "Heading 2",
    "text": "Heading 2"
  },
  {
    "objectID": "cv/index.html",
    "href": "cv/index.html",
    "title": "Curriculum vitæ",
    "section": "",
    "text": "Download CV"
  },
  {
    "objectID": "blog/2024/scraping-tutorial/index.html",
    "href": "blog/2024/scraping-tutorial/index.html",
    "title": "How to set up a web scraping environment with R and Selenium on Ubuntu",
    "section": "",
    "text": "Preface: Required software\nWhenever we want to do something great with a computer, we should have an operating system installed on it. In this tutorial, I use Ubuntu. I assume that some steps of my tutorial can also be transferred to other (especially Unix-based) operating systems. However, since I have not tried my tutorial on another operating system, I can only “guarantee”1 that my instructions will work for Ubuntu (I am using 22.04).\n1 If any problems occur despite my “guarantee”, plz don’t sue me - just write me an e-mail ^^Apart from an operating system, you also need an installation of R and the Docker Engine for Ubuntu. For writing code in R, I also recommend using a suitable IDE such as RStudio. If you already have these installed on your system, you can get started right away and pull a Docker image from Selenium.\nIf you don’t have R installed yet, you can install it from one of the CRAN mirrors in your country. You can install RStudio here. To install the Docker Engine on your machine, follow the instructions on the docker website. Simply copy the code given on the page into a terminal window and you should be good to go.\nAfter you have installed all the necessary software, we can move on to the main tool we will use to automate our browser: Selenium.\n\n\nPull docker image of Selenium\nSelenium is a software with which we can automate a browser. Many of the functions that Selenium contains are therefore very helpful for scraping content from a website. In this tutorial, we will not install Selenium directly on our system but run an image of Selenium within a Docker container.\nTo use Selenium within a container, we first have to pull an image of it in the Docker Engine. Depending on which browser you want to use, you can pull different versions for different browsers. Standalones are available for Firefox, Chrome, and Edge.\nSince I prefer to use Firefox, I’ll install a version that “uses” Firefox. With the following command in the terminal, you can pull the image:\n\ndocker pull selenium/standalone-firefox \n\n\n\n\n\n\n\nIssues with pulling the image?\n\n\n\nWhen I first tried to pull the image of Selenium, I got the error message: denied while trying to connect to the Docker daemon socket at unix\nThis error usually occurs when the user who runs the command to pull the image doesn’t have the necessary permissions to access the Docker daemon.\nFollow these steps to fix this problem permanently:\n\nAdd your user to the Docker group, which grants the user permission to interact with the Docker daemon. You can do this with the following command:\n\n\nsudo usermod -aG docker $USER \n\n\nAfter adding your user to the Docker group, you can run the following command to apply the changes to the current session:\n\n\nnewgrp docker\n\n\nAfter logging back in, you can verify that your user has access to Docker by running a Docker command without sudo, such as:\n\n\ndocker ps\n\n\nTry again to pull the docker image\n\n\n\n\n\nSet up a Docker container with Selenium\nAfter we have successfully pulled the Selenium image, we can now create a Docker container in which we run Selenium. To do so, use the following command in the terminal:\n\ndocker run -d -p 4444:4444 -p 7900:7900 --shm-size=\"2g\" selenium/standalone-firefox:latest \n\nThis command creates and starts a new Docker container based on the latest selenium/standalone-firefox image.\n\n Are you interested in what all the elements of this command mean? Click here to deconstruct it.\n \n\n  \n\n    docker run: This is the command used to create and start a new Docker container based on a specified Docker image\n    -d: This activates the \"detached mode.\" It tells Docker to run the container in the background and print the container ID\n    -p 4444:4444: This option maps port 4444 on the host machine to port 4444 inside the Docker container. This allows communication with the Selenium server from outside the container\n    -p 7900:7900: Similar to the previous option, this maps port 7900 on the host machine to port 7900 inside the Docker container. This port is used for Virtual Network Computing (VNC), which later allows us to see what our web scraper sees when navigating the Internet\n    --shm-size=\"2g\": This option sets the size of the shared memory segment for the container. In this case, it's set to 2 gigabytes (2g)\n    selenium/standalone-firefox:latest: This is the Docker image name and tag. It specifies the image to use for creating the container. Here, we are using the selenium/standalone-firefox image. The :latest tag indicates that it's using the latest version of the image.\n\n  \n\n\nWe have now managed to complete all preparations so that the web scraping can begin. But, before we can control our web scraper in R, we always need to make sure that the Docker container with the Selenium image is running.\n\n\nStart (and stop) the Selenium Docker container\nTo know whether our Docker container with Selenium is running, we should get an overview of all active Docker containers. To get an overview of all running containers, use this command in the terminal:\n\ndocker ps\n\nIf you also want to see containers that are not active at the moment but ran in the past, use the command:\n\ndocker ps -a\n\nThe second command comes in really handy if you want to start a container that you have already used in the past. Then you can simply copy the ID of the desired container from the overview of already used containers and then restart the old container with the following command:\n\ndocker start &lt;paste here the ID of the desired container&gt;\n\n\n\n\n\n\n\nStopping a container\n\n\n\nI advise you to stop the container after you finish the scraping. A running container requires computing power from your system and may cause other processes on the system to slow down.\nTo stop the container, use the command:\n\ndocker stop &lt;ID of running container&gt;\n\n\n\n\n\nControl the scraper in R\nOnce our Docker container with Selenium is running, we can start programming our web scraper in R. To do so, we use the R package selenium, which you can install as usual with install.packages(“selenium”) in R. This package contains numerous functions with which we can control the behavior of the automated browser. Explaining every function would probably require a separate blog entry - so I will only give a rough overview of the most important functions here:\n\nSeleniumSession$new(browser = \"firefox\") -&gt; object # starts the automated browser (here we need to assign the output of the function to an R object which we will use to address the browser in further instructions) \n\nobject$navigate(\"insert here an URL\") # navigates the browser to an URL\n\nobject$find_element # helps us to address a specific element on a webpage (for example with an XPATH)\n\nobject$get_page_source() # retrieves the source code of the webpage\n\n\n\n\n\n\n\nListen on ports to see what our scraper is doing\n\n\n\nIn most scraping projects, the automated browser has to click through various elements or make inputs at specific points on the website. Therefore, it is helpful to see how the automated browser navigates through a site so that we can see potential barriers and take them into account when programming the scraper. The great thing about Selenium is that it comes with a built-in VNC that allows us to do just that. All we need to do is type the following URL into a browser:\nhttp://localhost:7900/?autoconnect=1&resize=scale&password=secret\nIn addition, we can obtain an overview of the active automated browsers when listening on port 4444. This can be interesting if you want to run several scrapers simultaneously. To listen on port 4444, type in your browser:\nhttp://localhost:4444\n\n\nIf you have any questions or feedback about this blog post, please feel free to send me an e-mail. Otherwise, I wish you some happy scraping!"
  },
  {
    "objectID": "blog/2024/scraping-tutorial/index.html#footnotes",
    "href": "blog/2024/scraping-tutorial/index.html#footnotes",
    "title": "How to set up a web scraping environment with R and Selenium on Ubuntu",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nIf any problems occur despite my “guarantee”, plz don’t sue me - just write me an e-mail ^^↩︎"
  },
  {
    "objectID": "blog/2024/fixed-effects/index.html",
    "href": "blog/2024/fixed-effects/index.html",
    "title": "The magic of Fixed Effects regression",
    "section": "",
    "text": "In various sciences, researchers try to find causal relationships between two or more variables to answer their research questions. In environmental sciences and economics, for example, one current question is whether carbon prices have a causal influence on an economy’s greenhouse gas (GHG) emissions. To answer a question like this, usually a statistical model is set up to try to quantify the relationship between the predictors 1 and the dependent variable(s) 2.\n1 the variable from which an effect originates, e.g., the carbon price2 the variable that is impacted by the effect, e.g., GHG emissionsHowever, even if a statistical relationship is found, it is difficult to directly assume a causal relationship. Often there are numerous (potentially unobserved) variables that influence both the predictors and the dependent variable(s). Those variables are called confounders since they possibly confound a causal relationship. In the example from above, a country’s cultural conditions, for example, could be confounders as they might influence the carbon price as well as the GHG emissions.\n\n\n\n\n\n\nWhy could cultural factors in a country be confounding here?\n\n\n\nSuppose a country with a strong cultural commitment to climate protection implements or increases a carbon price and sees a reduction in GHG emissions. In this case, one might attribute the reduction solely to the carbon price. However, the reduction could also be driven by other complementary policies and measures (e.g., renewable energy subsidies, energy efficiency standards) that were adopted due to the same cultural commitment. In this case there would no causal relation between carbon prices and GHG emissions anymore.\n\n\nTo minimize the influence of such confounders, statisticians and social scientists have developed various methods. One of them is Fixed Effects regression."
  },
  {
    "objectID": "blog/2024/fixed-effects/index.html#why-all-this",
    "href": "blog/2024/fixed-effects/index.html#why-all-this",
    "title": "The magic of Fixed Effects regression",
    "section": "",
    "text": "In various sciences, researchers try to find causal relationships between two or more variables to answer their research questions. In environmental sciences and economics, for example, one current question is whether carbon prices have a causal influence on an economy’s greenhouse gas (GHG) emissions. To answer a question like this, usually a statistical model is set up to try to quantify the relationship between the predictors 1 and the dependent variable(s) 2.\n1 the variable from which an effect originates, e.g., the carbon price2 the variable that is impacted by the effect, e.g., GHG emissionsHowever, even if a statistical relationship is found, it is difficult to directly assume a causal relationship. Often there are numerous (potentially unobserved) variables that influence both the predictors and the dependent variable(s). Those variables are called confounders since they possibly confound a causal relationship. In the example from above, a country’s cultural conditions, for example, could be confounders as they might influence the carbon price as well as the GHG emissions.\n\n\n\n\n\n\nWhy could cultural factors in a country be confounding here?\n\n\n\nSuppose a country with a strong cultural commitment to climate protection implements or increases a carbon price and sees a reduction in GHG emissions. In this case, one might attribute the reduction solely to the carbon price. However, the reduction could also be driven by other complementary policies and measures (e.g., renewable energy subsidies, energy efficiency standards) that were adopted due to the same cultural commitment. In this case there would no causal relation between carbon prices and GHG emissions anymore.\n\n\nTo minimize the influence of such confounders, statisticians and social scientists have developed various methods. One of them is Fixed Effects regression."
  },
  {
    "objectID": "blog/2024/fixed-effects/index.html#fixed-effects-regression-in-a-nutshell",
    "href": "blog/2024/fixed-effects/index.html#fixed-effects-regression-in-a-nutshell",
    "title": "The magic of Fixed Effects regression",
    "section": "Fixed Effects regression in a nutshell",
    "text": "Fixed Effects regression in a nutshell\nFixed Effects regression is a regression model with which it is relatively easy to control for time-constant confounders. This means we can eliminate the impact of any time-stable variable that might influence our main effect of interest. The beautiful thing here is that we eliminate all time-constant confounders – regardless of whether we observed them or not. In our example, this means that we don’t have to care about any time-constant country characteristics 3 that could in any way confound the relationship between carbon prices and GHG emissions. We achieve this by only looking at the variation of the data within a unit of analysis.\n3 There are a lot! Just think about the different political systems, cultural and economic conditions, …In our example, this means that we examine how the carbon price affects GHG emissions solely within each country. Fixed Effects regression is therefore especially useful if we have several observations of a unit (e.g., a country) – e.g., if we have panel data. In the next section, I will explain how possible data for a Fixed Effects regression looks like and what happens under the hood."
  },
  {
    "objectID": "blog/2024/fixed-effects/index.html#how-does-a-fixed-effects-regression-work",
    "href": "blog/2024/fixed-effects/index.html#how-does-a-fixed-effects-regression-work",
    "title": "The magic of Fixed Effects regression",
    "section": "How does a Fixed Effects regression work?",
    "text": "How does a Fixed Effects regression work?\nAs already mentioned, a Fixed Effects regression is typicallyn done whenever we have repeated observations of or within the same unit (e.g., individuals, countries, …). In a dataset, this could look like this, for example:\n\n\n       Country Code Year Carbon_price Emission_per_capita\n1  Switzerland  CHE 2008     2.823883            5.852853\n2  Switzerland  CHE 2009     2.829404            5.628064\n3  Switzerland  CHE 2010     8.857406            5.758217\n4  Switzerland  CHE 2011     7.716105            5.180272\n5  Switzerland  CHE 2012     8.226921            5.283896\n6  Switzerland  CHE 2013     8.532256            5.338836\n7  Switzerland  CHE 2014    11.974738            4.791107\n8  Switzerland  CHE 2015    13.267141            4.676024\n9  Switzerland  CHE 2016    19.253305            4.679219\n10 Switzerland  CHE 2017    18.702318            4.516505\n\n\nHere we see that we have repeated measurements of the predictor carbon price and the dependent variable emissions per capita from Switzerland for several years.\nWith such a data set, a Fixed Effects regression model could now be set up, which could best be mathematically described with this equation:\n\\[\nY_{it} = \\alpha_i + \\beta X_{it}+ \\epsilon_{it}\n\\] Let’s break this formula down:\n\n\\(Y\\) stands for the value of the dependent variable for unit \\(i\\) at time point \\(t\\)\n\\(X\\) is the vector of all time-variant predictors 4\n\\(\\beta\\) is the matrix of the corresponding parameters of the predictors\n\\(\\alpha_i\\) represents unobserved time-stable individual effects - that’s why the subscript \\(t\\) is missing here\nThe formula also contains an error term \\(\\epsilon_{it}\\) that varies over time and across units\n\n4 in case we have just have one predictor, there is no vector or matrix neededIn our example, which includes only the predictor carbon price, the formula would be as follows:\n\\[\nemissions_{it} = \\alpha_i + \\beta price_{it} + \\epsilon_{it}\n\\]\nWith our model now established, the next step is to estimate the coefficient(s) \\(\\beta\\) to draw conclusions about the statistical relationships between the variables. There are numerous options for the statistical estimation of our model. In this article, I would like to take a closer look at two very common procedures: demeaning and adding a dummy variable for each unit. This will hopefully also help us to gain a deeper understanding of how Fixed Effects regression eliminates all time-stable confounders.\n\nStatistical estimation by adding a dummy variable for each unit\nOne of the numerically simplest variants for estimating the model is to construct a dummy variable for each unit (e.g., individuals, countries) 5. When doing this, each unit receives its own intercept. Each intercept then captures all the time-stable characteristics of that unit, meaning any systematic differences between units are absorbed by the individual intercepts.\n5 technically, we should omit the first unit to avoid multicollinearityLet’s take look at the relationship of carbon prices and GHG emissions in two selected countries of our dataset. For illustrative reasons, I select data from Switzerland and Denmark since 2007. When running a linear regression without adding any country-level dummy variables, the results look something like this:\n\n\n\n\n\n\n\n\n\nWe see one regression line describing our relationship between carbon prices and GHG emissions over all units in our dataset. If we now add a dummy variable for Switzerland and Denmark, the following happens:\n\n\n\n\n\n\n\n\n\nWe observe that each country now has its own intercept, which captures all time-stable characteristics specific to that country. In our example, these could include factors such as the political system, cultural norms, or economic conditions. So, by adding dummy variables for each country and constructing country-level intercepts, we accounted for all time-constant country-specific confounders.\nIn practice, I do not recommend estimating the model with dummy variables, as it becomes computationally intensive as soon as you have more than just a few units in your data set. In most situations, the preferred approach is demeaning.\n\n\nStatistical estimation with demeaning\nAnother way to extract the within-unit variation and control for time-stable confounders is demeaning or the within transformation. By demeaning, we subtract the mean of all observations for a given unit (e.g., an individual or country) from each observation of that unit. So, in our Switzerland dataset from above, this means we take all our observations of each year, calculate their mean, and then subtract that mean value from each observation. Manually performing this in R could look like this:\n\ndf_che |&gt; # data of Switzerland\n  mutate(Carbon_price = Carbon_price - mean(Carbon_price), # demeaning: subtracting mean from observations\n         Emission_per_capita = Emission_per_capita - mean(Emission_per_capita))\n\nThe new dataset that results from such a transformation would then look like this:\n\n\n       Country Code Year Carbon_price Emission_per_capita\n1  Switzerland  CHE 2008  -10.1669691          0.96726094\n2  Switzerland  CHE 2009  -10.1614481          0.74247134\n3  Switzerland  CHE 2010   -4.1334457          0.87262494\n4  Switzerland  CHE 2011   -5.2747467          0.29467964\n5  Switzerland  CHE 2012   -4.7639307          0.39830404\n6  Switzerland  CHE 2013   -4.4585957          0.45324334\n7  Switzerland  CHE 2014   -1.0161137         -0.09448536\n8  Switzerland  CHE 2015    0.2762893         -0.20956836\n9  Switzerland  CHE 2016    6.2624533         -0.20637336\n10 Switzerland  CHE 2017    5.7114663         -0.36908716\n\n\nTo enhance our intuition of how demeaning eliminates time-stable confounders, I would also like to revisit our equations from above. Let’s take a look at our basic Fixed Effects equation – where \\(\\alpha_i\\) stands for our time-constant 6 confounders:\n6 note the missing t in the subscript\\[\nY_{it} = \\alpha_i + \\beta X_{it}+ \\epsilon_{it}\n\\]\nBy demeaning, each observation at all time-points \\(t\\) is subtracted from the mean of all observations of unit \\(i\\):\n\\[\nY_{it} = \\alpha_i - \\bar \\alpha_i + \\beta (X_{it} - \\bar X_{i}) + \\epsilon_{it} - \\bar \\epsilon_{i}\n\\]\nSince \\(\\alpha_i\\) is time-constant and has thus the same value at different time points, its mean \\(\\bar \\alpha\\) is identical to \\(\\alpha_i\\) – which eliminates \\(\\alpha_i\\) from our equation. From this follows a transformed equation – in which our time-stable confounders have been removed:\n\\[\n\\ddot{Y_{it}}= \\ddot{\\beta X_{it}} + \\ddot{\\epsilon_{it}}\n\\]\n\n\n\n\n\n\nMore thoughts on this\n\n\n\nFor me, this process illustrates well what the basic idea of a Fixed Effects regression is. We know that time-stable variables of a unit do not change over time. If we perform demeaning and only look at the variation within a unit (where these possible variables remain stable over time) we don’t have to worry about these factors affecting our causal relationship – as they do not vary over time.\n\n\nAfter demeaning our data, we then simply calculate an OLS regression – and our Fixed Effects regression is completed. However, if we want to calculate a Fixed Effects model in R, we will not usually do this manually with the steps described here. There are good packages that do all the work for you and save you a lot of time. I can highly recommend the plm-package. You can find some information about it here."
  }
]