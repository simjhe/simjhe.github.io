[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Hello there!",
    "section": "",
    "text": "I am a Computational Social Science student with an excitement for statistics, programming and the use of computational methods in social scientific research. I have a professional background in quantitative media research and data analysis. Go check out my blog if you want to know what I’m working on at the moment. If you want to know more about me, check out my CV or my publications."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "blog/index.html#section",
    "href": "blog/index.html#section",
    "title": "Blog",
    "section": "2024",
    "text": "2024\n\n\n    \n                  \n            May 23, 2024\n        \n        \n            How to set up a web scraping environment with R and Selenium on Ubuntu\n            \n                \n                    R\n                \n                \n                    Web scraping\n                \n                \n                    Linux\n                \n            \n            Learn in this tutorial how to create a simple and beginner-friendly web scraping environment with Selenium and R on Ubuntu\n        \n        \n            \n        \n    \n\n\nNo matching items"
  },
  {
    "objectID": "blog/index.html",
    "href": "blog/index.html",
    "title": "Decode the Digits",
    "section": "",
    "text": "Welcome to my blog Decode the Digits. Add more description here."
  },
  {
    "objectID": "blog/article1_tba/article1.html",
    "href": "blog/article1_tba/article1.html",
    "title": "First test article",
    "section": "",
    "text": "CitationBibTeX citation:@online{2024,\n  author = {},\n  title = {First Test Article},\n  date = {2024-05-12},\n  doi = {10.59350/pe5s8-e0f47},\n  langid = {en}\n}\nFor attribution, please cite this work as:\n“First Test Article.” 2024. May 12, 2024. https://doi.org/10.59350/pe5s8-e0f47."
  },
  {
    "objectID": "blog/article1_tba/article1.html#title-1",
    "href": "blog/article1_tba/article1.html#title-1",
    "title": "First test article",
    "section": "",
    "text": "Here comes the content"
  },
  {
    "objectID": "blog/2024/article1_tba/index.html",
    "href": "blog/2024/article1_tba/index.html",
    "title": "How to set up a web scraping environment with R and Selenium on Ubuntu",
    "section": "",
    "text": "Preface: Required software\nIn this tutorial I use Ubuntu as the operating system. I assume that many of the steps described in the following can also be transferred to other (especially Unix-based) operating systems. However, since I have not yet tried it on other operating systems, I can only “guarantee” that my instructions will work for Ubuntu (I am using 22.04).\nApart from an operating system, you also need an installation of R and the Docker Engine for Ubuntu. For writing code in R, I also recommend using a suitable IDE such as RStudio. If you already have these installed on your system, you can get started right away and pull a Docker image from Selenium.\nIf you don’t have R installed yet, you can install it from one of the CRAN mirrors in your country. You can install RStudio here. To install the Docker Engine on your machine, follow the instructions on the docker website. Simply copy the code given on the page into a terminal window and you should be good to go.\nAfter you have installed all the necessary software, we can move on to the actual tool we will use to automate our browser: Selenium.\n\n\nPull docker image of Selenium\nSelenium is a software with which we can automate a browser. Many of the functions that Selenium contains are therefore extremely helpful for scraping content from websites. In this tutorial, we will not install Selenium directly on our system, but run an image of Selenium within a Docker container.\nTo be able to use Selenium within a container, we first have to pull an image of it in the Docker Engine. Depending on which browser you want to use, you can pull different versions for different browsers. Standalones are available for Firefox, Chrome, and Edge.\nSince I personally prefer to use Firefox, I’ll install a version that “uses” firefox. With the following command in the terminal, you can pull the image:\n\ndocker pull selenium/standalone-firefox \n\n\n\n\n\n\n\nIssues with pulling the image?\n\n\n\nWhen I first tried to pull the image of Selenium, I got the error message: denied while trying to connect to the Docker daemon socket at unix\nThis error usually occurs when the user that runs the command to pull the image doesn’t have the necessary permissions to access the Docker daemon.\nFollow these steps to fix this problem permanently:\n\nAdd your user to the Docker group, which grants the user permission to interact with the Docker daemon. You can do this with the following command:\n\n\nsudo usermod -aG docker $USER \n\n\nAfter adding your user to the Docker group, you can run the following command to apply the changes to the current session: newgrp docker\nAfter logging back in, you can verify that your user has access to Docker by running a Docker command without sudo, such as: docker ps\nTry again to pull the docker image\n\n\n\n\n\nSet up a Docker container with Selenium\nAfter we have successfully pulled the Selenium image, we can now create a Docker container in which we run Selenium. To do so, use the following command in the terminal:\n\ndocker run -d -p 4444:4444 -p 7900:7900 --shm-size=\"2g\" selenium/standalone-firefox:latest \n\nThis command creates and starts a new Docker container based on the latest selenium/standalone-firefox image.\n\n Are you interested in what all the elements of this command mean? Click here to deconstruct it.\n \n\n  \n\n    docker run: This is the command used to create and start a new Docker container based on a specified Docker image\n    -d: This activates the \"detached mode.\" It tells Docker to run the container in the background and print the container ID\n    -p 4444:4444: This option maps port 4444 on the host machine to port 4444 inside the Docker container. This allows communication with the Selenium server from outside the container\n    -p 7900:7900: Similar to the previous option, this maps port 7900 on the host machine to port 7900 inside the Docker container. This port is used for Virtual Network Computing (VNC), which later allows us to see what our web scraper sees when navigating the Internet\n    --shm-size=\"2g\": This option sets the size of the shared memory segment for the container. In this case, it's set to 2 gigabytes (2g)\n    selenium/standalone-firefox:latest: This is the Docker image name and tag. It specifies the image to use for creating the container. Here, we are using the selenium/standalone-firefox image. The :latest tag indicates that it's using the latest version of the image.\n\n  \n\n\nWe have now managed to complete all preparations so that the web scraping can begin. Before we can control our web scraper in R, however, we always have to start the Docker container with the Selenium image first.\n\n\nStart (and stop) the Selenium Docker container\nTo know whether we need to start our Docker container with Selenium, it usually makes sense to get an overview of all active Docker containers. To get an overview of all running containers, use this command in the terminal:\n\ndocker ps\n\nIf you also want to see containers that are not active at the moment but ran in the past, use the command:\n\ndocker ps -a\n\nThe second command comes really handy if you want to start a container that you have already used in the past. Because then you can simply copy the ID of the desired container from the overview of already used containers and then restart the old container with the following command:\n\ndocker start &lt;here ID of desired container&gt;\n\n\n\n\n\n\n\nStopping a container\n\n\n\nI advise you to stop the container after you finished the scraping. A running container requires computing power from your system and may cause other processes on the system to slow down.\nTo stop the container, use the command:\n\ndocker stop &lt;ID of running container&gt;\n\n\n\n\n\nControl the scraper in R\nOnce our Docker container with Selenium is running, we can start programming our web scraper in R. To do so, we use the R package selenium, which you can install as usual with install.packages(“selenium”) in R. This package contains numerous functions with which we can control the behavior of the automated browser. Explaining every function would probably require a separate blog entry - so I will only give a rough overview of the most important functions here:\n\nSeleniumSession$new(browser = \"firefox\") -&gt; object # starts the automated browser (here we need to assign the output of the function to an R object which we will use to address the browser in further instructions) \n\nobject$navigate(\"insert here an URL\") # navigates the browser to an URL\n\nobject$find_element # helps us to address a specific element on a webpage (for example with an XPATH)\n\nobject$get_page_source() # retrieves the source code of the webpage\n\n\n\n\n\n\n\nListen on ports to see what our scraper is doing\n\n\n\nIn most scraping projects, the automated browser has to click through various elements or make inputs at specific points on the website. Therefore, it is helpful to see how the automated browser navigates through a site so that potential barriers can be easily observed and taken into account when programming the scraper. The great thing about Selenium is that it comes with a built-in VNC that allows us to do just that. All we need to do is type the following URL into a browser:\nhttp://localhost:7900/?autoconnect=1&resize=scale&password=secret\nIn addition, we can obtain an overview of the active automated browsers when listening on port 4444. This can be interesting if you want to run several scrapers simultaneously. To listen on port 4444, type in your browser:\nhttp://localhost:4444\n\n\nIf you have any questions or feedback about this blog post, please feel free to send me an email. Otherwise, I wish you a happy scraping!"
  },
  {
    "objectID": "publications.html",
    "href": "publications.html",
    "title": "Publications",
    "section": "",
    "text": "In the following, you’ll find an overview of reports and articles I contributed to as an author."
  },
  {
    "objectID": "publications.html#section",
    "href": "publications.html#section",
    "title": "Publications",
    "section": "2023",
    "text": "2023\n\nCousseran, L., Lauber, A., Herrmann, S., & Brüggen, N. (2023). Compass: Artificial intelligence and competence 2023. Attitudes, actions and competence development in the context of AI. Munich: kopaed. https://doi.org/10.5281/ZENODO.10058587\nHerrmann, S., Lauber, A., Cousseran, L., & Brüggen, N. (2023). Compass: Artificial intelligence and competence 2022. User types of interaction with AI. Munich: kopaed. https://doi.org/10.5281/ZENODO.8245676\nHerrmann, S., Cousseran, L., Tausche, S., Pfaff-Rüdiger, S., & Brüggen, N. (2023). Compass: Artificial intelligence and competence 2022. Media use and attitudes towards AI. Munich: kopaed. https://doi.org/10.5281/ZENODO.6900551"
  },
  {
    "objectID": "publications.html#section-1",
    "href": "publications.html#section-1",
    "title": "Publications",
    "section": "2022",
    "text": "2022\n\nSchober, M., Lauber, A., Bruch, L., Herrmann, S., & Brüggen, N. (2022). “What I like comes to me”. Young people’s competencies in using algorithmic recommendation systems. Munich: kopaed. https://doi.org/10.5281/ZENODO.7437429\nPfaff-Rüdiger, S., Herrmann, S., Cousseran, L., & Brüggen, N. (2022). Compass: Artificial intelligence and competence 2022. Knowledge and action in the context of AI. Munich: kopaed. https://doi.org/10.5281/ZENODO.6668912\n\n(All publication titles were translated from German into English)"
  },
  {
    "objectID": "blog/2024/article1_tba/index.html#heading-2",
    "href": "blog/2024/article1_tba/index.html#heading-2",
    "title": "First test article",
    "section": "",
    "text": "docker run -d -p 4444:4444 -p 7900:7900 --shm-size=\"2g\" selenium/standalone-firefox:latest \n\n\n\n\n\n\nPossible issues with pulling image for the first time\n\n\n\n\nAdd text here"
  },
  {
    "objectID": "blog/2024/article1_tba/index.html#heading-2-1",
    "href": "blog/2024/article1_tba/index.html#heading-2-1",
    "title": "First test article",
    "section": "Heading 2",
    "text": "Heading 2"
  },
  {
    "objectID": "cv/index.html",
    "href": "cv/index.html",
    "title": "Curriculum vitæ",
    "section": "",
    "text": "Download CV"
  },
  {
    "objectID": "blog/2024/scraping-tutorial/index.html",
    "href": "blog/2024/scraping-tutorial/index.html",
    "title": "How to set up a web scraping environment with R and Selenium on Ubuntu",
    "section": "",
    "text": "Preface: Required software\nWhenever we want to do something great with a computer, we should have an operating system installed on it. In this tutorial, I use Ubuntu. I assume that some steps of my tutorial can also be transferred to other (especially Unix-based) operating systems. However, since I have not tried my tutorial on another operating system, I can only “guarantee”1 that my instructions will work for Ubuntu (I am using 22.04).1 If any problems occur despite my “guarantee”, plz don’t sue me - just write me an e-mail ^^\nApart from an operating system, you also need an installation of R and the Docker Engine for Ubuntu. For writing code in R, I also recommend using a suitable IDE such as RStudio. If you already have these installed on your system, you can get started right away and pull a Docker image from Selenium.\nIf you don’t have R installed yet, you can install it from one of the CRAN mirrors in your country. You can install RStudio here. To install the Docker Engine on your machine, follow the instructions on the docker website. Simply copy the code given on the page into a terminal window and you should be good to go.\nAfter you have installed all the necessary software, we can move on to the main tool we will use to automate our browser: Selenium.\n\n\nPull docker image of Selenium\nSelenium is a software with which we can automate a browser. Many of the functions that Selenium contains are therefore very helpful for scraping content from a website. In this tutorial, we will not install Selenium directly on our system but run an image of Selenium within a Docker container.\nTo use Selenium within a container, we first have to pull an image of it in the Docker Engine. Depending on which browser you want to use, you can pull different versions for different browsers. Standalones are available for Firefox, Chrome, and Edge.\nSince I prefer to use Firefox, I’ll install a version that “uses” Firefox. With the following command in the terminal, you can pull the image:\n\ndocker pull selenium/standalone-firefox \n\n\n\n\n\n\n\nIssues with pulling the image?\n\n\n\nWhen I first tried to pull the image of Selenium, I got the error message: denied while trying to connect to the Docker daemon socket at unix\nThis error usually occurs when the user who runs the command to pull the image doesn’t have the necessary permissions to access the Docker daemon.\nFollow these steps to fix this problem permanently:\n\nAdd your user to the Docker group, which grants the user permission to interact with the Docker daemon. You can do this with the following command:\n\n\nsudo usermod -aG docker $USER \n\n\nAfter adding your user to the Docker group, you can run the following command to apply the changes to the current session: newgrp docker\nAfter logging back in, you can verify that your user has access to Docker by running a Docker command without sudo, such as: docker ps\nTry again to pull the docker image\n\n\n\n\n\nSet up a Docker container with Selenium\nAfter we have successfully pulled the Selenium image, we can now create a Docker container in which we run Selenium. To do so, use the following command in the terminal:\n\ndocker run -d -p 4444:4444 -p 7900:7900 --shm-size=\"2g\" selenium/standalone-firefox:latest \n\nThis command creates and starts a new Docker container based on the latest selenium/standalone-firefox image.\n\n Are you interested in what all the elements of this command mean? Click here to deconstruct it.\n \n\n  \n\n    docker run: This is the command used to create and start a new Docker container based on a specified Docker image\n    -d: This activates the \"detached mode.\" It tells Docker to run the container in the background and print the container ID\n    -p 4444:4444: This option maps port 4444 on the host machine to port 4444 inside the Docker container. This allows communication with the Selenium server from outside the container\n    -p 7900:7900: Similar to the previous option, this maps port 7900 on the host machine to port 7900 inside the Docker container. This port is used for Virtual Network Computing (VNC), which later allows us to see what our web scraper sees when navigating the Internet\n    --shm-size=\"2g\": This option sets the size of the shared memory segment for the container. In this case, it's set to 2 gigabytes (2g)\n    selenium/standalone-firefox:latest: This is the Docker image name and tag. It specifies the image to use for creating the container. Here, we are using the selenium/standalone-firefox image. The :latest tag indicates that it's using the latest version of the image.\n\n  \n\n\nWe have now managed to complete all preparations so that the web scraping can begin. But, before we can control our web scraper in R, we always need to make sure that the Docker container with the Selenium image is running.\n\n\nStart (and stop) the Selenium Docker container\nTo know whether our Docker container with Selenium is running, we should get an overview of all active Docker containers. To get an overview of all running containers, use this command in the terminal:\n\ndocker ps\n\nIf you also want to see containers that are not active at the moment but ran in the past, use the command:\n\ndocker ps -a\n\nThe second command comes in really handy if you want to start a container that you have already used in the past. Then you can simply copy the ID of the desired container from the overview of already used containers and then restart the old container with the following command:\n\ndocker start &lt;paste here the ID of the desired container&gt;\n\n\n\n\n\n\n\nStopping a container\n\n\n\nI advise you to stop the container after you finish the scraping. A running container requires computing power from your system and may cause other processes on the system to slow down.\nTo stop the container, use the command:\n\ndocker stop &lt;ID of running container&gt;\n\n\n\n\n\nControl the scraper in R\nOnce our Docker container with Selenium is running, we can start programming our web scraper in R. To do so, we use the R package selenium, which you can install as usual with install.packages(“selenium”) in R. This package contains numerous functions with which we can control the behavior of the automated browser. Explaining every function would probably require a separate blog entry - so I will only give a rough overview of the most important functions here:\n\nSeleniumSession$new(browser = \"firefox\") -&gt; object # starts the automated browser (here we need to assign the output of the function to an R object which we will use to address the browser in further instructions) \n\nobject$navigate(\"insert here an URL\") # navigates the browser to an URL\n\nobject$find_element # helps us to address a specific element on a webpage (for example with an XPATH)\n\nobject$get_page_source() # retrieves the source code of the webpage\n\n\n\n\n\n\n\nListen on ports to see what our scraper is doing\n\n\n\nIn most scraping projects, the automated browser has to click through various elements or make inputs at specific points on the website. Therefore, it is helpful to see how the automated browser navigates through a site so that we can see potential barriers and take them into account when programming the scraper. The great thing about Selenium is that it comes with a built-in VNC that allows us to do just that. All we need to do is type the following URL into a browser:\nhttp://localhost:7900/?autoconnect=1&resize=scale&password=secret\nIn addition, we can obtain an overview of the active automated browsers when listening on port 4444. This can be interesting if you want to run several scrapers simultaneously. To listen on port 4444, type in your browser:\nhttp://localhost:4444\n\n\nIf you have any questions or feedback about this blog post, please feel free to send me an email. Otherwise, I wish you some happy scraping!"
  },
  {
    "objectID": "blog/2024/scraping-tutorial/index.html#footnotes",
    "href": "blog/2024/scraping-tutorial/index.html#footnotes",
    "title": "How to set up a web scraping environment with R and Selenium on Ubuntu",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nIf any problems occur despite my “guarantee”, plz don’t sue me - just write me an e-mail ^^↩︎"
  }
]