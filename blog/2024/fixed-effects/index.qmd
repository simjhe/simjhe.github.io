---
title: "The magic of Fixed Effects regression"
date: 2024-09-02
description: "In this article I give a short introduction to Fixed Effects regression. I try to give an intuition of how it works and in which scenarios it is useful"
image: "img/teaser-img_fixed-effects.png"
categories:
  - Econometrics
  - Statistical methods
  - R
resources:
  - "img/*"
format:
  html: 
    shift-heading-level-by: 0
toc-location: left
execute: 
  eval: true
  warning: false
---

```{r}
#| include: false

knitr::opts_chunk$set()

# load data
df_price <- read.csv("data/emissions-weighted-carbon-price.csv")
df_emissions <- read.csv("data/co-emissions-per-capita.csv")

# packages
library(tidyverse)

# only look for data in 2021
df_price <- df_price |>
  rename(Carbon_price = Average.price.on.emissions.covered.by.a.carbon.tax..weighted.by.the.share.of.the.country.s.CO..emissions,
         Country = Entity)

df_emissions <- df_emissions |>
  rename(Emission_per_capita = Annual.CO..emissions..per.capita.,
         Country = Entity) |> 
  select(!Code)

# join data
df_joined <- inner_join(x = df_price, y = df_emissions, by = c("Country", "Year"))
```

## Why all this?

In various sciences, researchers try to find causal relationships between two or more variables to answer their research questions. In environmental sciences and economics, for example, one current question is whether carbon prices have a causal influence on an economy’s greenhouse gas (GHG) emissions. To answer a question like this, usually a statistical model is set up to try to quantify the relationship between the predictors ^[the variable from which an effect originates, e.g., the carbon price] and the dependent variable(s) ^[the variable that is impacted by the effect, e.g., GHG emissions]. 

However, even if a statistical relationship is found, it is difficult to directly assume a causal relationship. Often there are numerous (potentially unobserved) variables that influence both the predictors and the dependent variable(s). Those variables are called confounders since they possibly confound a causal relationship. In the example from above, a country’s cultural conditions, for example, could be confounders as they might influence the carbon price as well as the GHG emissions. 

::: {.callout-tip}
## Why could cultural factors in a country be confounding here?
Suppose a country with a strong cultural commitment to climate protection implements or increases a carbon price and sees a reduction in CO2 emissions. In this case, one might attribute the reduction solely to the carbon price. However, the reduction could also be driven by other complementary policies and measures (e.g., renewable energy subsidies, energy efficiency standards) that were adopted due to the same cultural commitment. In this case there would no causal relation between CO2 prices and CO2 anymore.
:::

To minimize the influence of such confounders, statisticians and social scientists have developed various methods. One of them is Fixed Effects regression.

## Fixed Effects regression in a nutshell

Fixed Effects regression is a regression model with which it is relatively easy to control for time-constant confounders. This means we can eliminate the impact of any time-stable variable that might influence our main effect of interest. The beautiful thing here is that we eliminate all time-constant confounders – regardless of whether we observed them or not. In our example, this means that we don’t have to care about any time-constant country characteristics ^[There are a lot! Just think about the different political systems, cultural and economic conditions, …] that could in any way confound the relationship between carbon prices and GHG emissions. We achieve this by only looking at the variation of the data within a unit of analysis. 

In our example, this means that we examine how the carbon price affects GHG emissions solely within each country. Fixed Effects regression is therefore only possible if we have several observations of a unit (e.g., a country) – i.e., if we have panel data. In the next section, I will explain how possible data for a Fixed Effects regression looks like and what happens under the hood. 

## How does a Fixed Effects regression work?

As already mentioned, a Fixed Effects regression can be performed whenever we have repeated observations of or within the same unit (e.g., individuals, countries, ...). In a dataset, this could look like this, for example:

```{r}
#| output: true
#| echo: false
df_joined |> 
  filter(Code == "CHE", Carbon_price > 0) |> 
  head(10)
```

Here we see that we have repeated measurements of the predictor *carbon price* and the dependent variable *emissions* from Switzerland for several years.

With such a data set, a Fixed Effects regression model could now be set up, which could best be mathematically described with this equation:

$$
Y_{it} = \alpha_i + \beta X_{it}+ \epsilon_{it}
$$
Let's break this formula down:

- $Y$ stands for the value of the dependent variable for unit $i$ at time point $t$
- $X$ is the vector of all time-variant predictors ^[in case we have just have one predictor, there is no vector or matrix needed]
- $\beta$ is the matrix of the corresponding parameters of the predictors
- $\alpha_i$ represents unobserved time-stable individual effects - that’s why the subscript $t$ is missing here
- The formula also contains an error term $\epsilon_{it}$ that varies over time and across units

In our example, which includes only the predictor *carbon price*, the formula would be as follows:

$$
emissions_{it} = \alpha_i + \beta price_{it} + \epsilon_{it}
$$

With our model now established, the next step is to estimate the coefficient(s) $\beta$ to draw conclusions about the statistical relationships between the variables. There are numerous options for the statistical estimation of our model. In this article, I would like to take a closer look at two very common procedures: *demeaning* and adding a *dummy variable* for each unit. This will also help us gain a deeper understanding of how Fixed Effects regression eliminates all time-stable confounders. 

### Statistical estimation by adding a dummy variable for each unit

One of the numerically simplest variants for estimating the model is to construct a dummy variable for each unit (e.g., individuals, countries) ^[technically, we should omit the first unit to avoid multicollinearity]. When doing this, each unit receives its own intercept. Each intercept then captures all the time-stable characteristics of that unit, meaning any systematic differences between units are absorbed by the individual intercepts. 

Let’s take look at the relationship of carbon prices and GHG emissions in three selected countries of our dataset. For illustrative reasons, I select data from Switzerland, Denmark, and Norway since 2007. When running a linear regression over the data without adding any country-level dummy variables, the results look something like this:

```{r}
#| output: true
#| echo: false
df_joined |> 
  filter((Code == "CHE" | Code == "DNK" | Code == "NOR") & Year >= 2007) -> df_dummies

lm(Emission_per_capita ~ Carbon_price + factor(Code), data = df_dummies) -> lm_dummies

df_dummies <- cbind(df_dummies, pred = predict(lm_dummies))

df_dummies |> 
  ggplot(mapping = aes(x = Carbon_price,
                       y = Emission_per_capita)) +
  geom_point() + 
  geom_smooth(method='lm', se = F) +
  labs(x = "Carbon prices",
       y = "GHG emissions per capita")
```

The data is spread out and – based on our regression line – we see a positive relationship between carbon prices and GHG emissions. If we now add a dummy variable for Switzerland, Denmark, and Norway, the following happens: 

```{r}
#| output: true
#| echo: false
df_dummies |> 
  ggplot(mapping = aes(x = Carbon_price,
                       y = Emission_per_capita,
                       colour = Country)) +
  geom_point() + 
  geom_smooth(method='lm', se = F, fullrange = T,
              mapping = (aes(y = pred))) +
  labs(x = "Carbon prices",
       y = "GHG emissions per capita")
```

We see that each country has now its own intercept. Each intercept captures all time-stable characteristics of the respective country. In our example, these could include factors such as the political system, cultural norms, or economic conditions. So, by adding dummy variables for each country and constructing country-level intercepts, we controlled for time-constant confounders – which in our example has visibly led to different results than an OLS regression.^[Pretty cool, right?]

In practice, I do not recommend estimating the model with dummy variables, as it becomes computationally intensive as soon as you have more than just a few units in your data set. In most situations, the preferred approach is demeaning.

### Statistical estimation with demeaning

Another way to extract the within-unit variation and control for time-stable confounders is *demeaning* or the *within transformation*. By demeaning, we subtract the mean of all observations for a given unit (e.g., an individual or country) from each observation of that unit. So, in our Switzerland dataset from above, this means we take all our observations of each year, calculate their mean, and then subtract that mean value from each observation:

```{r}
#| output: true
#| echo: true
df_joined |> 
  filter(Code == "CHE" & Year > 2007 & Carbon_price > 0) |> 
  mutate(Carbon_price = Carbon_price - mean(Carbon_price),
         Emission_per_capita = Emission_per_capita - mean(Emission_per_capita)) |> 
  head(10)
```

To enhance our understanding of how demeaning eliminates time-stable confounders we can also revisit our equations from above. Let’s take a look at our Fixed Effects equation – where $\alpha_i$ stands for our time-constant ^[note the missing t in the subscript] confounders:

$$
Y_{it} = \alpha_i + \beta X_{it}+ \epsilon_{it}
$$

By demeaning, each observation at all time-points $t$ is subtracted from the mean of all observations of unit $i$:

$$
Y_{it} = \alpha_i - \bar \alpha_i + \beta (X_{it} - \bar X_{i}) + \epsilon_{it} - \bar \epsilon_{i}
$$

Since $\alpha_i$ is time-constant and has thus the same value at different time points, its mean $\bar \alpha$ is identical to $\alpha_i$ – which eliminates $\alpha_i$ from our equation. From this follows a transformed equation – in which our time-stable confounders have been removed:

$$
\ddot{Y_{it}}= \ddot{\beta X_{it}} + \ddot{\epsilon_{it}}
$$

::: {.callout-tip}
## More thoughts on this?
For me, this process illustrates well what the basic idea of a Fixed Effects regression is. We know that time-stable variables of a unit do not change over time. If we perform demeaning and only look at the variation within a unit (where these possible variables remain stable over time) we don't have to worry about these factors affecting our causal relationship – as they do not vary over time. 
:::

After demeaning our data, we then simply calculate an OLS regression – and our Fixed Effects regression is successfully completed.