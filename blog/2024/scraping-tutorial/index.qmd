---
title: "How to set up a web scraping environment with R and Selenium on Ubuntu"
date: 2024-05-12
description: "Learn in this tutorial how to create a simple and beginner-friendly web scraping environment with Selenium and R on Ubuntu"
image: "img/test.png"
open-graph:
  image: "img/card.png"
categories:
  - R
  - Web scraping
  - Linux
resources:
  - "img/*"
format:
  html: 
    shift-heading-level-by: 1
toc-location: left
execute: 
  eval: false
---

```{r}
#| include: false

knitr::opts_chunk$set()
```

## Preface: Required software

In this tutorial I use Ubuntu as the operating system. I assume that many of the steps described in the following can also be transferred to other (especially Unix-based) operating systems. However, since I have not yet tried it on other operating systems, I can only “guarantee” that my instructions will work for Ubuntu (I am using 22.04).

Apart from an operating system, you also need an installation of R and the Docker Engine for Ubuntu. For writing code in R, I also recommend using a suitable IDE such as RStudio. If you already have these installed on your system, you can get started right away and pull a Docker image from Selenium.

If you don't have R installed yet, you can install it from one of the [CRAN mirrors in your country](https://cran.r-project.org/mirrors.html). You can install RStudio [here](https://posit.co/download/rstudio-desktop/). To install the Docker Engine on your machine, follow the instructions on the [docker website](https://docs.docker.com/engine/install/ubuntu/#install-using-the-repository). Simply copy the code given on the page into a terminal window and you should be good to go.

After you have installed all the necessary software, we can move on to the actual tool we will use to automate our browser: Selenium.

## Pull docker image of Selenium

Selenium is a software with which we can automate a browser. Many of the functions that Selenium contains are therefore extremely helpful for scraping content from websites. In this tutorial, we will not install Selenium directly on our system, but run an image of Selenium within a Docker container.

To be able to use Selenium within a container, we first have to pull an image of it in the Docker Engine. Depending on which browser you want to use, you can pull different versions for different browsers. Standalones are available for [Firefox](https://hub.docker.com/r/selenium/standalone-firefox), [Chrome](https://hub.docker.com/r/selenium/standalone-chrome), and [Edge](https://hub.docker.com/r/selenium/standalone-edge).

Since I personally prefer to use Firefox, I’ll install a version that “uses” firefox. With the following command in the terminal, you can pull the image:

```{bash}
docker pull selenium/standalone-firefox 
```

::: {#downloads .callout-tip}
## Issues with pulling the image?

When I first tried to pull the image of Selenium, I got the error message: denied while trying to connect to the Docker daemon socket at unix

This error usually occurs when the user that runs the command to pull the image doesn't have the necessary permissions to access the Docker daemon.

Follow these steps to fix this problem permanently:

1.  Add your user to the Docker group, which grants the user permission to interact with the Docker daemon. You can do this with the following command:

```{bash}
sudo usermod -aG docker $USER 
```

2.  After adding your user to the Docker group, you can run the following command to apply the changes to the current session: newgrp docker

3.  After logging back in, you can verify that your user has access to Docker by running a Docker command without sudo, such as: docker ps

4.  Try again to pull the docker image
:::

## Set up a Docker container with Selenium

After we have successfully pulled the Selenium image, we can now create a Docker container in which we run Selenium. To do so, use the following command in the terminal:

```{bash}
docker run -d -p 4444:4444 -p 7900:7900 --shm-size="2g" selenium/standalone-firefox:latest 
```

This command creates and starts a new Docker container based on the latest selenium/standalone-firefox image. 

```{=html}
<a data-bs-toggle="collapse" data-bs-target="#collapseElement" aria-expanded="false" aria-controls="collapseElement">
 <b>Are you interested in what all the elements of this command mean? Click here to deconstruct it.</b>
</a> <br>
<div class="collapse" id="collapseElement">
  <div class="card card-body">
<ul>
    <li><strong>docker run</strong>: This is the command used to create and start a new Docker container based on a specified Docker image</li>
    <li><strong>-d</strong>: This activates the "detached mode." It tells Docker to run the container in the background and print the container ID</li>
    <li><strong>-p 4444:4444</strong>: This option maps port 4444 on the host machine to port 4444 inside the Docker container. This allows communication with the Selenium server from outside the container</li>
    <li><strong>-p 7900:7900</strong>: Similar to the previous option, this maps port 7900 on the host machine to port 7900 inside the Docker container. This port is used for Virtual Network Computing (VNC), which later allows us to see what our web scraper sees when navigating the Internet</li>
    <li><strong>--shm-size="2g"</strong>: This option sets the size of the shared memory segment for the container. In this case, it's set to 2 gigabytes (2g)</li>
    <li><strong>selenium/standalone-firefox:latest</strong>: This is the Docker image name and tag. It specifies the image to use for creating the container. Here, we are using the selenium/standalone-firefox image. The :latest tag indicates that it's using the latest version of the image.</li>
</ul>
  </div>
</div>
<br>
```

We have now managed to complete all preparations so that the web scraping can begin. Before we can control our web scraper in R, however, we always have to start the Docker container with the Selenium image first.

## Start (and stop) the Selenium Docker container

To know whether we need to start our Docker container with Selenium, it usually makes sense to get an overview of all active Docker containers. To get an overview of all running containers, use this command in the terminal:

```{bash}
docker ps
```

If you also want to see containers that are not active at the moment but ran in the past, use the command:

```{bash}
docker ps -a
```

The second command comes really handy if you want to start a container that you have already used in the past. Because then you can simply copy the ID of the desired container from the overview of already used containers and then restart the old container with the following command:

```{bash}
docker start <here ID of desired container>
```

::: {#downloads .callout-tip}
## Stopping a container

I advise you to stop the container after you finished the scraping. A running container requires computing power from your system and may cause other processes on the system to slow down. 

To stop the container, use the command:

```{bash}
docker stop <ID of running container>
```
:::

## Control the scraper in R 

Once our Docker container with Selenium is running, we can start programming our web scraper in R. To do so, we use the R package *selenium*, which you can install as usual with ```install.packages(“selenium”)``` in R. This package contains numerous functions with which we can control the behavior of the automated browser. Explaining every function would probably require a separate blog entry - so I will only give a rough overview of the most important functions here: 

```{r}
SeleniumSession$new(browser = "firefox") -> object # starts the automated browser (here we need to assign the output of the function to an R object which we will use to address the browser in further instructions) 

object$navigate("insert here an URL") # navigates the browser to an URL

object$find_element # helps us to address a specific element on a webpage (for example with an XPATH)

object$get_page_source() # retrieves the source code of the webpage
```

::: {#downloads .callout-tip}
## Listen on ports to see what our scraper is doing

In most scraping projects, the automated browser has to click through various elements or make inputs at specific points on the website. Therefore, it is helpful to see how the automated browser navigates through a site so that potential barriers can be easily observed and taken into account when programming the scraper. The great thing about Selenium is that it comes with a built-in VNC that allows us to do just that. All we need to do is type the following URL into a browser:

<http://localhost:7900/?autoconnect=1&resize=scale&password=secret> 

In addition, we can obtain an overview of the active automated browsers when listening on port 4444. This can be interesting if you want to run several scrapers simultaneously. To listen on port 4444, type in your browser:

<http://localhost:4444>
:::

If you have any questions or feedback about this blog post, please feel free to send me an email. Otherwise, I wish you a happy scraping!
